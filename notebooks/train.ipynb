{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073849a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_synthesis.datasets.dataset import VAEMaskDataset\n",
    "\n",
    "\n",
    "dataset = VAEMaskDataset()\n",
    "\n",
    "train_dataset, test_dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1d61c",
   "metadata": {},
   "source": [
    "recon, mu, logvar = vae(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a6bb0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgan_synthesis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmask_vae_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VAE, kl_divergence\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgan_synthesis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# TODO: add live matplotlib plotting\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# TODO: add function to completely process kaggle dataset into fully processed 2D slices so can work on PC\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Instantiate data loaders\u001b[39;00m\n\u001b[32m     18\u001b[39m dataset = Dataset()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from gan_synthesis.mask_vae_models.encoder import Encoder\n",
    "from gan_synthesis.mask_vae_models.decoder import Decoder\n",
    "from gan_synthesis.mask_vae_models.vae import VAE, kl_divergence\n",
    "from gan_synthesis.datasets.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# TODO: add live matplotlib plotting\n",
    "# TODO: add function to completely process kaggle dataset into fully processed 2D slices so can work on PC\n",
    "\n",
    "\n",
    "# Instantiate data loaders\n",
    "dataset = Dataset()\n",
    "train_set, test_set = dataset.split(0.8)\n",
    "train_loader = DataLoader(train_set, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=2, shuffle=True)\n",
    "\n",
    "# Instantiate models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Before the loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    for _, seg in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        seg = seg.to(device)\n",
    "\n",
    "        recon, mu, logvar = model(seg)\n",
    "        kld_loss = kl_divergence(mu, logvar)\n",
    "        ce_loss = criterion(recon, seg)\n",
    "        loss = ce_loss + 5e-3 * kld_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, seg in tqdm(test_loader, desc=\"Test\", leave=False):\n",
    "            seg = seg.to(device)\n",
    "            recon, mu, logvar = model(seg)\n",
    "            kld_loss = kl_divergence(mu, logvar)\n",
    "            ce_loss = criterion(recon, seg)\n",
    "            loss = ce_loss + 5e-3 * kld_loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(recon, dim=1)\n",
    "            correct += (pred == seg).sum().item()\n",
    "            total += seg.numel()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    acc = correct / total\n",
    "    val_accuracies.append(acc)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b115eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_synthesis.preprocessing.transforms import read_data\n",
    "seg= read_data(1, \"seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "346d9326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23182454",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Instantiate models\u001b[39;00m\n\u001b[32m     29\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m encoder = \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m decoder = Decoder().to(device)\n\u001b[32m     32\u001b[39m model = VAE(encoder, decoder).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from gan_synthesis.mask_vae_models.encoder import Encoder\n",
    "from gan_synthesis.mask_vae_models.decoder import Decoder\n",
    "from gan_synthesis.mask_vae_models.vae import VAE, kl_divergence\n",
    "from gan_synthesis.datasets.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable live plotting\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot([], [], label=\"Train Loss\")\n",
    "line2, = ax.plot([], [], label=\"Val Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Live Training/Validation Loss\")\n",
    "ax.legend()\n",
    "\n",
    "# Instantiate data loaders\n",
    "dataset = Dataset()\n",
    "train_set, test_set = dataset.split(0.8)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Instantiate models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    for _, seg in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
    "        seg_input = seg.to(torch.float32).to(device)\n",
    "        seg_target = seg.to(device)\n",
    "        print(seg_target.shape)\n",
    "\n",
    "        recon, mu, logvar = model(seg_input)\n",
    "        kld_loss = kl_divergence(mu, logvar)\n",
    "        ce_loss = criterion(recon, seg_target)\n",
    "        loss = ce_loss + 5e-3 * kld_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, seg in tqdm(test_loader, desc=\"Validation\", leave=False):\n",
    "            seg = seg.to(device)\n",
    "            seg_input = seg.to(torch.float32).to(device)\n",
    "            seg_target = seg.to(device)\n",
    "\n",
    "            recon, mu, logvar = model(seg_input)\n",
    "            kld_loss = kl_divergence(mu, logvar)\n",
    "            ce_loss = criterion(recon, seg_target)\n",
    "            loss = ce_loss + 5e-3 * kld_loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(recon, dim=1)\n",
    "            correct += (pred == seg_target).sum().item()\n",
    "            total += seg.numel()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    acc = correct / total\n",
    "    val_accuracies.append(acc)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {acc:.4f}\")\n",
    "\n",
    "    # Update live plot\n",
    "    line1.set_xdata(range(1, len(train_losses)+1))\n",
    "    line1.set_ydata(train_losses)\n",
    "    line2.set_xdata(range(1, len(val_losses)+1))\n",
    "    line2.set_ydata(val_losses)\n",
    "\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Keep final plot open\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9cda0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4883424, Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "model = Encoder()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "dtype = next(model.parameters()).dtype\n",
    "print(f\"Total parameters: {total_params}, Dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea60ce87",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m encoder = \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f593024a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddedDllDirectory('C:\\\\Users\\\\zmirz\\\\gan-project\\\\.pixi\\\\envs\\\\default\\\\Library\\\\bin')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.add_dll_directory(r\"C:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\Library\\bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9916af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zmirz\\gan-project\\.pixi\\envs\\default\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49db5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# test again\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d7f8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/zmirz/gan-project/notebooks')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\zmirz\\gan-project\\processed_data\\contrast_slice_316.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3381eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    return sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104229cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7427140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_count(vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
